class MultiArmedBandit{constructor(){this.numArms=3,this.armDistributions=this.setupArms(),this.trueMeans=this.calculateTrueMeans()}setupArms(){return[{rewards:[-1,1],probabilities:[.5,.5]},{rewards:[1,0],probabilities:[.4,.6]},{rewards:[2,-1,0],probabilities:[.3,.1,.6]}]}calculateTrueMeans(){return this.armDistributions.map((t=>t.rewards.reduce(((a,s,r)=>a+s*t.probabilities[r]),0)))}pullArm(t){if(t<0||t>=this.numArms)throw new Error(`Invalid arm index: ${t}. Must be 0, 1, or 2.`);const a=this.armDistributions[t],s=Math.random();let r=0;for(let t=0;t<a.probabilities.length;t++)if(r+=a.probabilities[t],s<=r)return a.rewards[t];return a.rewards[a.rewards.length-1]}getArmInfo(t){if(t<0||t>=this.numArms)throw new Error(`Invalid arm index: ${t}`);return{armIndex:t,rewards:this.armDistributions[t].rewards,probabilities:this.armDistributions[t].probabilities,trueMean:this.trueMeans[t]}}getAllArmsInfo(){return this.armDistributions.map(((t,a)=>this.getArmInfo(a)))}getOptimalArm(){return this.trueMeans.indexOf(Math.max(...this.trueMeans))}}class BanditSession{constructor(t=50){this.bandit=new MultiArmedBandit,this.maxPulls=t,this.reset()}reset(){this.totalPulls=0,this.armPulls=[0,0,0],this.armRewards=[0,0,0],this.pullHistory=[]}pullArm(t){const a=t-1;if(this.totalPulls>=this.maxPulls)return{success:!1,message:`Maximum pulls (${this.maxPulls}) reached!`};try{const s=this.bandit.pullArm(a);return this.totalPulls++,this.armPulls[a]++,this.armRewards[a]+=s,this.pullHistory.push({arm:t,reward:s}),{success:!0,reward:s,arm:t,statistics:this.getStatistics()}}catch(t){return{success:!1,message:t.message}}}getStatistics(){const t=this.armPulls.map(((t,a)=>t>0?Math.round(this.armRewards[a]/t*1e3)/1e3:0));return{total_pulls:this.totalPulls,max_pulls:this.maxPulls,arm_pulls:this.armPulls,arm_rewards:this.armRewards,arm_means:t,pull_history:this.pullHistory.slice(-10)}}canPull(){return this.totalPulls<this.maxPulls}}class EpsilonGreedyAgent{constructor(t=3,a=.1){this.numArms=t,this.epsilon=a,this.reset()}reset(){this.armCounts=new Array(this.numArms).fill(0),this.armValues=new Array(this.numArms).fill(0),this.totalPulls=0,this.totalReward=0,this.pullHistory=[]}selectArm(){if(Math.random()<this.epsilon){return{arm:Math.floor(Math.random()*this.numArms),actionType:"explore"}}return{arm:this.armValues.indexOf(Math.max(...this.armValues)),actionType:"exploit"}}update(t,a){this.armCounts[t]++,this.armValues[t]+=(a-this.armValues[t])/this.armCounts[t],this.totalPulls++,this.totalReward+=a,this.pullHistory.push({arm:t,reward:a,armValues:[...this.armValues],armCounts:[...this.armCounts]})}getEstimates(){return{arm_estimates:[...this.armValues],arm_counts:[...this.armCounts],total_reward:this.totalReward,average_reward:this.totalPulls>0?this.totalReward/this.totalPulls:0,epsilon:this.epsilon}}}class BanditSimulator{constructor(){this.bandit=new MultiArmedBandit,this.banditData=null}async loadBanditData(){if(this.banditData)return this.banditData;try{const t=await fetch("./data/bandit_data.json");if(!t.ok)throw new Error(`HTTP error! status: ${t.status}`);return this.banditData=await t.json(),console.log("Bandit data loaded successfully"),this.banditData}catch(t){throw console.error("Failed to load bandit data:",t),t}}async demonstrateExploration(t=100){await this.loadBanditData();const a=this.banditData.exploration_simulation;return{simulation_steps:a.simulation_steps.map((t=>({step:t.step,arm:t.arm,action_type:t.action_type,reward:t.reward,estimates:t.arm_values_after||[0,0,0],counts:t.arm_counts_after||[0,0,0],total_reward:t.total_reward||0}))),summary:{total_reward:a.summary.total_reward,average_reward:a.summary.average_reward,epsilon:a.summary.epsilon,final_estimates:a.summary.final_arm_values}}}async demonstrateTrained(t=100){await this.loadBanditData();const a=this.banditData.trained_simulation;return{simulation_steps:a.simulation_steps.map((t=>({step:t.step,arm:t.arm,action_type:t.action_type,reward:t.reward,estimates:t.arm_values,confidence:t.confidence}))),summary:{optimal_arm_selections:a.summary.optimal_arm_selections,optimal_percentage:a.summary.optimal_percentage,epsilon:a.summary.epsilon,initial_estimates:a.summary.initial_arm_values}}}async revealDistributions(){await this.loadBanditData();const t=this.banditData.environment;return{success:!0,arms_info:t.arms.map((t=>({arm:t.arm_index+1,rewards:t.rewards,probabilities:t.probabilities,expected_reward:t.true_mean}))),optimal_arm:t.optimal_arm+1,true_means:t.true_means}}simulateExplorationStep(t,a){const s=t.selectArm(),r=s.arm,e=s.actionType,i=this.bandit.pullArm(r),l={step:a,arm:r,action_type:e,reward:i,arm_values_before:[...t.armValues],arm_counts_before:[...t.armCounts]};return t.update(r,i),l.arm_values_after=[...t.armValues],l.arm_counts_after=[...t.armCounts],l.total_reward=t.totalReward,l.average_reward=t.totalReward/(a+1),l}}