<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Attention Mechanism</title>
    <script src="../scripts/mathjax-config.js"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="../styles.css">
    <link rel="stylesheet" href="../scripts/demotab.css">
    <link rel="stylesheet" href="../demo-common.css">
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <div class="iframe-content">
        <div class="container">

        <div class="info-tabs">
            <div class="tab-header">
            </div>
            <div class="tab-content">
                <div class="tab-panel active" data-tab-title="Introduction">
                    <h3>Attention Mechanism</h3>
                    <p>The attention mechanism is a fundamental component of modern neural networks, particularly in natural language processing and computer vision. It allows models to focus on relevant parts of the input when processing information.</p>

                    <p>This interactive demo visualizes how attention works with:</p>
                    <ul>
                        <li><strong>Query (Q)</strong>: What each token "asks for"</li>
                        <li><strong>Key (K)</strong>: What each token "offers"</li>
                        <li><strong>Value (V)</strong>: The actual information content</li>
                        <li><strong>Attention Weights</strong>: How much focus each token receives</li>
                    </ul>

                    <p>The demo uses causal masking (lower triangular) typical in language models, where tokens can only attend to previous positions.</p>
                </div>

                <div class="tab-panel" data-tab-title="Theory">
                    <h3>Mathematical Foundation</h3>

                    <p>The attention mechanism computes a weighted average of value vectors, where weights are determined by the compatibility between queries and keys:</p>

                    <div class="equation-block">
                        $$\text{Attention}(Q,K,V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$
                    </div>

                    <p><strong>Step-by-step process:</strong></p>
                    <ol>
                        <li><strong>Linear Transformations:</strong> $Q = XW_Q$, $K = XW_K$, $V = XW_V$</li>
                        <li><strong>Compute Scores:</strong> $S = \frac{QK^T}{\sqrt{d_k}}$ (scaled dot-product)</li>
                        <li><strong>Apply Mask:</strong> Set future positions to $-\infty$ for causal attention</li>
                        <li><strong>Normalize:</strong> $A = \text{softmax}(S)$ (attention weights sum to 1)</li>
                        <li><strong>Weighted Sum:</strong> $O = AV$ (final output)</li>
                    </ol>

                    <p><strong>Why scaling by $\sqrt{d_k}$?</strong> This prevents the dot products from becoming too large, which would push the softmax into regions with extremely small gradients.</p>
                </div>

                <div class="tab-panel" data-tab-title="Instructions">
                    <h3>How to Use This Demo</h3>

                    <ul>
                        <li><strong>Select a sequence</strong> from the dropdown to see how attention processes different inputs</li>
                        <li><strong>Hover over input tokens</strong> to see their corresponding Q, K, V vectors highlighted in the matrices</li>
                        <li><strong>Hover over output embeddings</strong> to trace how they were computed from attention weights and values</li>
                        <li><strong>Click "Randomize Weights"</strong> to see different random weight initializations and their effects</li>
                        <li><strong>Observe the causal mask</strong> in the attention weights matrix - notice how future positions are masked out</li>
                    </ul>

                    <p><strong>Color Coding:</strong></p>
                    <ul>
                        <li><em style="color: #4a90e2;">Blue (Q):</em> Query vectors - what each token "asks for"</li>
                        <li><em style="color: #7cb342;">Green (K):</em> Key vectors - what each token "offers"</li>
                        <li><em style="color: #f44336;">Red (V):</em> Value vectors - information each token contains</li>
                        <li><em style="color: #4dbeee;">Light Blue (O):</em> Output embeddings - final attention results</li>
                    </ul>
                </div>

                <div class="tab-panel" data-tab-title="Tips">
                    <h3>Understanding the Visualization</h3>

                    <p><strong>Educational Notes:</strong></p>
                    <ul>
                        <li>This demo shows <em>untrained attention with random initialization</em></li>
                        <li>It's designed to build <em>visual intuition</em> for attention mechanics, not realistic outputs</li>
                        <li>In real applications, these weights are learned through training</li>
                        <li>Fixed dimensions ($d_{model} = 4$, $d_k = 4$) allow for clear visualization</li>
                    </ul>

                    <p><strong>Key Insights:</strong></p>
                    <ul>
                        <li>Notice how the attention weights matrix is lower triangular due to causal masking</li>
                        <li>Each row in the attention weights sums to 1.0 (softmax normalization)</li>
                        <li>Different weight initializations can dramatically change the attention patterns</li>
                        <li>The output is always a weighted combination of the value vectors</li>
                    </ul>

                    <p><strong>Real-world Applications:</strong></p>
                    <ul>
                        <li>Language models (GPT, BERT) use multi-head attention</li>
                        <li>Machine translation systems rely heavily on attention</li>
                        <li>Computer vision transformers use attention for image processing</li>
                    </ul>
                </div>
            </div>
        </div>

        <!-- Horizontal Controls -->
        <div class="horizontal-controls" style="display: flex; gap: 20px; align-items: center; justify-content: center; margin: 20px 0; padding: 15px; background: #f8f9fa; border-radius: 8px; flex-wrap: wrap;">
            <div style="display: flex; align-items: center; gap: 10px;">
                <label for="sequence-select" style="font-weight: bold; white-space: nowrap;">Input Sequence:</label>
                <select id="sequence-select" style="padding: 6px 10px; border: 1px solid #ddd; border-radius: 4px;">
                    <option value="simple">The cat sat</option>
                    <option value="greeting">Hello world today</option>
                    <option value="question">What is attention</option>
                    <option value="longer">The quick brown fox jumps</option>
                </select>
            </div>

            <button class="generate-data-btn" id="randomize-weights-btn" style="white-space: nowrap;">Randomize Weights</button>
        </div>

        <div class="demo-area">
            <div class="plot-container" style="width: 100%;">

                <!-- Input Sequence & QKV Matrices -->
                <div class="attention-container">
                    <h4 style="margin: 0 0 15px 0; text-align: center; color: #2c3e50;">Input Sequence & Query, Key, Value Matrices</h4>
                    <div style="display: flex; align-items: flex-start; gap: 30px;">
                        <!-- Input Sequence -->
                        <div style="flex: 0 0 200px;">
                            <h5 style="margin: 0 0 10px 0; color: #2c3e50; text-align: center;">Input Tokens</h5>
                            <div class="vertical-sequence-container" id="input-sequence">
                                <!-- Tokens will be dynamically generated -->
                            </div>
                        </div>

                        <!-- QKV Matrices -->
                        <div style="flex: 1; display: flex; gap: 20px;">
                            <div class="vertical-matrix-section">
                                <div class="matrix-title query">Query (Q)</div>
                                <canvas id="q-canvas" width="160" height="220"></canvas>
                            </div>
                            <div class="vertical-matrix-section">
                                <div class="matrix-title key">Key (K)</div>
                                <canvas id="k-canvas" width="160" height="220"></canvas>
                            </div>
                            <div class="vertical-matrix-section">
                                <div class="matrix-title value">Value (V)</div>
                                <canvas id="v-canvas" width="160" height="220"></canvas>
                            </div>
                        </div>
                    </div>
                    <div class="math-explanation" id="qkv-explanation">
                        $Q = XW_Q$, $K = XW_K$, $V = XW_V$ where $X$ is the input embedding matrix
                    </div>
                </div>

                <!-- Attention Weights, Values & Output -->
                <div class="attention-container" id="attention-container">
                    <h4 style="margin: 0 0 15px 0; text-align: center; color: #2c3e50;">Attention Weights, Values & Output</h4>
                    <div style="display: flex; align-items: flex-start; gap: 20px;">
                        <div style="flex: 0 0 31%; text-align: center;">
                            <div class="matrix-title" style="background: #666; color: white; font-weight: bold; margin-bottom: 10px; padding: 5px; border-radius: 4px;">Attention Weights (A)</div>
                            <canvas id="attention-canvas" width="350" height="350" style="width: 100%; max-width: 350px;"></canvas>
                        </div>
                        <div style="flex: 0 0 31%; text-align: center;">
                            <div class="matrix-title value">Values (V)</div>
                            <canvas id="v-attention-canvas" width="350" height="350" style="width: 100%; max-width: 350px;"></canvas>
                        </div>
                        <div style="flex: 0 0 31%; text-align: center;">
                            <div class="matrix-title" style="background: rgb(77, 190, 238); color: white; font-weight: bold; margin-bottom: 10px; padding: 5px; border-radius: 4px;">Output (O)</div>
                            <div class="vertical-sequence-container" id="output-sequence">
                                <!-- Output tokens will be dynamically generated -->
                            </div>
                        </div>
                    </div>
                    <div style="display: flex; gap: 20px;">
                        <div class="math-explanation" id="attention-explanation" style="width: 31%; margin: 10px 0 0 0;">
                            Attention weights: $A = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)$ where each row sums to 1
                        </div>
                        <div class="math-explanation" id="output-explanation" style="width: 62%; margin: 10px 0 0 0;">
                            Output: $O = AV$ - weighted combination of value vectors
                        </div>
                    </div>
                </div>
            </div>
        </div>

        </div>
    </div>

    <script src="../scripts/demotab.js"></script>
    <script src="script.js"></script>
</body>
</html>